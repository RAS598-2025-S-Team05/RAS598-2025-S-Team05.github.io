{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UR5 &amp; TurtleBot Collaboration using ROS2","text":"","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#project-overview","title":"Project Overview","text":"<ul> <li>Team Number: 05</li> <li>Team Members: Asrith Pandreka, Varun Karthik, Anjali Notani</li> <li>Semester and Year: Spring 2025</li> <li>University: Arizona State University</li> <li>Class: RAS598 Experimentation and Deployment of Robotics</li> <li>Advising Faculty: Dr. Daniel Aukes</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#introduction","title":"Introduction","text":"<p>Our project explores the collaborative capabilities of a UR5 robotic arm and a TurtleBot using ROS2. The central research question is: How can we design an integrated system where the UR5 and TurtleBot work together to perform complex tasks such as object manipulation and navigation in dynamic environments? Specifically, the UR5 will pick and place objects at predefined locations, while the TurtleBot will autonomously navigate to these locations, verify the object's presence, and optionally pick it up. This project aims to optimize inter-robot communication, improve object detection accuracy, and ensure robust navigation in real-world scenarios. By leveraging ROS2's advanced features, such as MoveIt2 for motion planning and Nav2 for navigation, we aim to create a seamless collaboration between the two robots, paving the way for applications in industrial automation, logistics, and beyond.</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#example-figure","title":"Example figure","text":"","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#sensor-integration","title":"Sensor Integration","text":"<p>Sensor data is critical for enabling the UR5 and TurtleBot to perform their tasks effectively. The UR5\u2019s RGB-D camera will be used for object detection, with the ur5_vision_node processing the camera feed using YOLO/OpenCV to detect objects and publish their positions to the /object_detected topic. The gripper sensors on the UR5 will provide feedback to ensure successful grasping and releasing of objects, which will be used in the ur5_pick_place_node to confirm task completion. On the TurtleBot, the LiDAR will be used for SLAM (Simultaneous Localization and Mapping) and navigation, with the turtlebot_navigation_node subscribing to LiDAR data to create a map of the environment and plan paths. The TurtleBot\u2019s RGB-D camera will verify the presence of the object after the UR5 places it, with the turtlebot_vision_node processing the camera feed and publishing verification results to the /object_verified topic. ROS2 topics will facilitate communication between the UR5 and TurtleBot, ensuring seamless integration of sensor data.</p> <p>During testing, sensors will be used to validate the functionality of each component and ensure the system works as expected. In simulation (Gazebo), simulated sensor data, such as LiDAR and RGB-D camera feeds, will be used to test the UR5 and TurtleBot in a virtual environment. Mock object detection nodes will simulate object placement and verification. In hardware testing, the UR5\u2019s RGB-D camera and gripper sensors will be tested for accuracy and reliability, while the TurtleBot\u2019s LiDAR and RGB-D camera will be tested for navigation and object verification. Sensor data will be logged and analyzed to identify and fix issues. Integration testing will involve testing the communication between the UR5 and TurtleBot using real sensor data, such as the UR5 placing an object and the TurtleBot navigating to and verifying the object.</p> <p>In the final demonstration, sensors will play a crucial role in showcasing the system\u2019s capabilities. The UR5\u2019s RGB-D camera will detect an object in the environment, and the gripper will pick it up, with sensor feedback confirming successful grasping and placement. The TurtleBot\u2019s LiDAR will create a map of the environment and navigate to the object\u2019s location, while the RGB-D camera will verify the object\u2019s presence and confirm task completion. Real-time sensor data, such as object detection results and navigation status, will be displayed on a web-based UI, allowing the audience to see how the robots use sensor data to perform their tasks. If sensors detect an issue, such as an object not being found or a navigation failure, the system will handle it gracefully, with the TurtleBot reattempting navigation or signaling an error. This approach ensures that sensor data is effectively utilized throughout the project lifecycle, from development to final demonstration.</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#interaction","title":"Interaction","text":"","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#influencing-robot-behavior","title":"Influencing Robot Behavior","text":"<p>We will influence the behavior of the UR5 and TurtleBot through a combination of pre-programmed tasks, real-time sensor feedback, and user interaction. The robots will follow a predefined workflow, such as the UR5 picking and placing objects and the TurtleBot navigating to verify the object. Their behavior will dynamically adjust based on sensor data (e.g., object detection, navigation status) and user inputs. For example, if the TurtleBot fails to verify an object, it will reattempt navigation or signal an error. ROS2 nodes, such as turtlebot_control_node and ur5_pick_place_node, will process sensor data and execute decision-making logic to ensure smooth operation.</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#interfaces-for-viewinginteraction-and-data-storage","title":"Interfaces for Viewing,Interaction and Data Storage","text":"<p>We will develop a web-based dashboard for real-time monitoring and control, displaying camera feeds, navigation maps, and system logs. Users can start/stop tasks, view sensor data, and access performance metrics. Additionally, we will use ROS2 CLI tools for launching nodes, debugging, and testing. Sensor data and system logs will be stored using ROSbag and visualized with tools like PlotJuggler or custom scripts. These interfaces will enable seamless interaction, real-time monitoring, and post-processing analysis.</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#control-and-autonomy","title":"Control and Autonomy","text":"<p>We will connect sensor feedback to the controllers and higher-level decision-making processes through ROS2 nodes and real-time communication. For the UR5, sensor data from the RGB-D camera and gripper will be processed by the ur5_vision_node and ur5_pick_place_node to detect objects and execute pick-and-place tasks. For the TurtleBot, LiDAR and RGB-D camera data will be used by the turtlebot_navigation_node and turtlebot_vision_node for navigation and object verification. Higher-level decision-making will occur in nodes like turtlebot_control_node, which will subscribe to topics like /object_verified to decide the next action. This modular approach ensures that sensor feedback directly influences both low-level control (e.g., motion planning) and high-level decision-making (e.g., task sequencing).</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#preperation-needs","title":"Preperation Needs","text":"<p>To be successful, we need a strong understanding of ROS2 fundamentals, including node communication, topic publishing/subscribing, and service calls. We also need expertise in robot motion planning (e.g., MoveIt2 for UR5) and navigation (e.g., Nav2 for TurtleBot). Knowledge of object detection algorithms (e.g., YOLO, OpenCV) and sensor integration (e.g., LiDAR, RGB-D cameras) is essential. Additionally, familiarity with simulation tools like Gazebo and debugging techniques for ROS2 systems will be critical.</p> <p>In class, we need coverage of ROS2 advanced concepts, such as multi-robot communication and DDS, as well as practical sessions on motion planning and navigation. Hands-on tutorials for sensor integration and object detection would also be highly beneficial. These topics will provide the foundational knowledge and skills required to implement and debug our project effectively.  </p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#final-demonstration-showcasing-the-project","title":"Final Demonstration: Showcasing the Project","text":"","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#how-will-you-demonstrate-your-work-in-class","title":"How Will You Demonstrate Your Work in Class?","text":"<p>We will demonstrate the collaboration between the UR5 and TurtleBot in a controlled environment. The UR5 will pick up an object from a predefined location and place it at a target location. The TurtleBot will then navigate to the object\u2019s location using SLAM, verify the object\u2019s presence using its RGB-D camera, and optionally pick it up. Real-time performance data, such as object detection results and navigation status, will be displayed on a web-based dashboard. This will showcase the seamless integration of the two robots and their ability to work together autonomously.</p>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#what-resources-will-you-need","title":"What Resources Will You Need?","text":"","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#hardware","title":"Hardware:","text":"<ul> <li>UR5 robotic arm with a gripper and RGB-D camera.</li> <li>TurtleBot3 with LiDAR and RGB-D camera.</li> <li>A PC for UR5 control and a Raspberry Pi/Jetson Nano for TurtleBot.</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#software","title":"Software:","text":"<ul> <li>ROS2 Humble/Foxy.</li> <li>MoveIt2, Nav2, Gazebo, and OpenCV/YOLO.</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#environment","title":"Environment:","text":"<ul> <li>A small, obstacle-free area for the demonstration.</li> <li>Objects for the UR5 to pick and place.</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#classroom-setup-requirements","title":"Classroom Setup Requirements","text":"<ul> <li>Space: A clear area of at least 3m x 3m for the robots to operate.</li> <li>Power Supply: Access to power outlets for the UR5, TurtleBot, and computing devices.</li> <li>Projector/Screen: To display the web-based dashboard for real-time monitoring.</li> <li>Wi-Fi: Stable internet connection for ROS2 communication and dashboard access.</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#handling-variability-in-the-environment","title":"Handling Variability in the Environment","text":"<p>The robots are designed to handle variability through adaptive algorithms and error recovery mechanisms:</p> <ul> <li>UR5: If the object is not detected or the gripper fails to grasp, the UR5 will retry the operation or signal an error.</li> <li>TurtleBot: If the environment changes (e.g., new obstacles), the TurtleBot will use SLAM to update its map and replan its path. If object verification fails, it will reattempt navigation or signal an error.</li> <li>Real-Time Adjustments: The system will dynamically adjust based on sensor feedback, ensuring robustness in varying conditions.</li> </ul>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#testing-evaluation-plan","title":"Testing &amp; Evaluation Plan","text":"<ol> <li>Unit Testing:</li> <li> <p>Test individual components (e.g., UR5 pick-and-place, TurtleBot navigation) in isolation using Gazebo and real hardware.</p> </li> <li> <p>Integration Testing:</p> </li> <li> <p>Test the full workflow in simulation and real-world environments to ensure seamless communication between the UR5 and TurtleBot.</p> </li> <li> <p>Performance Metrics:</p> </li> <li> <p>Measure task completion time, object detection accuracy, and navigation success rate.</p> </li> <li> <p>Error Handling:</p> </li> <li> <p>Evaluate the system\u2019s ability to recover from errors (e.g., object not found, navigation failure).</p> </li> <li> <p>Final Demonstration:</p> </li> <li>Execute the full workflow in class, showcasing the robots\u2019 ability to collaborate and adapt to changes.</li> </ol>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"#impact","title":"Impact","text":"<p>This project has a significant impact on both personal learning and course development. By integrating the UR5 and TurtleBot using ROS2, we gain hands-on experience in collaborative robotics, real-time communication, and autonomous decision-making. This drives us to learn new material, such as advanced ROS2 concepts (e.g., DDS, multi-agent systems), object detection algorithms (e.g., YOLO, OpenCV), and motion planning (e.g., MoveIt2). The project also encourages exploration of sensor integration (e.g., LiDAR, RGB-D cameras) and simulation tools (e.g., Gazebo), which are essential for robotics development.</p> <p>For course development, this project serves as a practical example of multi-robot collaboration, providing a framework for future students to build upon. It highlights the importance of modular design, error handling, and real-time adaptability, which are critical skills in robotics. Additionally, the project demonstrates the potential of ROS2 in industrial and research applications, inspiring further exploration of automation and AI-driven robotics. Overall, this work not only enhances our technical expertise but also contributes to a richer, more applied learning experience for the entire course.</p> <p># Impact</p> <pre><code>Dr. Aukes will serve as our project advisor, providing mentorship, technical guidance, and access to specialized hardware and resources. We will rely on Dr. Aukes for expertise in ROS2 development, robot motion planning, and system integration. Specifically, we need access to the UR5 robotic arm, TurtleBot3, and sensors (e.g., LiDAR, RGB-D cameras) available in the lab. Additionally, we will seek guidance on best practices for collaborative robotics and error handling strategies.\n</code></pre>","tags":["robotics","ROS2","UR5","TurtleBot"]},{"location":"Timeline/","title":"Project Timeline","text":""},{"location":"Timeline/#weekly-milestones","title":"Weekly Milestones","text":"Week Hardware Integration Interface Development Sensor Processing Controls &amp; Autonomy Week 7 (March 1-7) Setup UR5 and TurtleBot hardware Design initial UI mockups for monitoring Capture initial sensor data (camera, LiDAR, IMU) Basic ROS2 node setup for communication Week 8 (March 8-14) Test UR5 pick-and-place mechanism Develop ROS2-based interaction dashboard Implement object detection using OpenCV Integrate sensor feedback into navigation system Week 9 (March 15-21) Tune external camera and UR5 grasping accuracy Display real-time sensor data on UI Test LiDAR-based mapping and obstacle detection Implement LiDAR-based path planning Week 10 (March 22-28) Establish wireless communication between UR5 &amp; TurtleBot Enable remote UI control for robot monitoring Merge object detection data with LiDAR maps Implement TurtleBot navigation using Nav2 Week 11 (March 29-April 4) Conduct real-world object placement tests Enhance visualization of navigation path Improve SLAM performance for precise localization Implement adaptive obstacle avoidance Week 12 (April 5-11) Coordinate multi-robot operation Finalize GUI for monitoring &amp; debugging Improve real-time sensor fusion for accuracy Validate navigation and target localization Week 13 (April 12-18) Final hardware testing and debugging Ensure UI usability for real-time monitoring Cross-validate sensor data for robustness Final integration of UR5 and TurtleBot behavior Week 14 (April 19-25) Optimize data transmission &amp; system response time Finalize UI for seamless operation Conduct robustness tests for mapping accuracy Refine object approach and handling behaviors Week 15 (April 26-28) Final debugging and error corrections Ensure UI responsiveness and usability Cross-validate object localization and mapping Ensure seamless execution of task sequences Week 16 (April 29-30) Conduct final demo &amp; evaluation Document findings and prepare report Analyze performance and accuracy metrics Present results and future scope recommendations"},{"location":"about/","title":"About Team05","text":"<p>Welcome to the official website of Team05! We are a group of passionate students dedicated to advancing robotics technology through experimentation and deployment.</p>","tags":["team","mission","project"]},{"location":"about/#our-mission","title":"Our Mission","text":"<p>Our mission is to innovate and push the boundaries of robotics by developing cutting-edge systems and deploying them in real-world scenarios. We aim to create solutions that are both practical and impactful.</p>","tags":["team","mission","project"]},{"location":"about/#meet-the-team","title":"Meet the Team","text":"","tags":["team","mission","project"]},{"location":"about/#asrith-pandreka","title":"Asrith Pandreka","text":"<ul> <li>Role: Team Lead</li> <li>Expertise: Robotics, Embedded Systems</li> <li>Bio: Asrith is a master's student with a passion for robotics and automation. He oversees the team's projects and ensures everything runs smoothly.</li> </ul>","tags":["team","mission","project"]},{"location":"about/#varun-karthik","title":"Varun Karthik","text":"<ul> <li>Role: HW Design Developer</li> <li>Expertise: Mechatronics, AI</li> <li>Bio: Varun specializes in developing intelligent algorithms for robotics systems. He is responsible for the software architecture of our projects.</li> </ul>","tags":["team","mission","project"]},{"location":"about/#anjali-notani","title":"Anjali Notani","text":"<ul> <li>Role: Hardware Engineer</li> <li>Expertise: Circuit Design, Sensors</li> <li>Bio: Anjali focuses on designing and building the hardware components of our robotics systems. She ensures that our designs are both efficient and reliable.</li> </ul>","tags":["team","mission","project"]},{"location":"about/#our-projects","title":"Our Projects","text":"","tags":["team","mission","project"]},{"location":"about/#project-1-autonomous-navigation","title":"Project 1: Autonomous Navigation","text":"<ul> <li>Description: Developing an autonomous robot capable of navigating complex environments.</li> <li>Technologies: ROS, SLAM, Computer Vision</li> <li>Status: In Progress</li> </ul>","tags":["team","mission","project"]},{"location":"about/#project-2-robotic-arm","title":"Project 2: Robotic Arm","text":"<ul> <li>Description: Building a robotic arm for industrial automation.</li> <li>Technologies: Arduino, Servo Motors, Kinematics</li> <li>Status: Completed</li> </ul>","tags":["team","mission","project"]},{"location":"about/#contact-us","title":"Contact Us","text":"<p>If you have any questions or would like to collaborate with us, feel free to reach out:</p> <ul> <li>GitHub: Team05 GitHub</li> <li>Website: Team05 Website</li> </ul> <p>\"Innovation is the key to the future.\" \u2013 Team05</p>","tags":["team","mission","project"]},{"location":"charts/","title":"Project Implementation Process","text":""},{"location":"charts/#flowchart-project-workflow","title":"Flowchart: Project Workflow","text":"<pre><code>graph TD\n  A[Start] --&gt; B{UR5 Object Placement};\n  B --&gt;|Object Placed| C[Detect Position with Camera];\n  C --&gt;|Extract Coordinates| D[Send Data to TurtleBot];\n  D --&gt;|Coordinates Received| E[SLAM Navigation Initiated];\n  E --&gt;|Path Planning| F[TurtleBot Navigates to Object];\n  F --&gt;|Verifies Object Location| G[End];</code></pre>"},{"location":"charts/#sequence-diagram-communication-between-ur5-and-turtlebot","title":"Sequence Diagram: Communication Between UR5 and TurtleBot","text":"<pre><code>sequenceDiagram\nautonumber\nparticipant UR5\nparticipant Camera\nparticipant TurtleBot\n\nUR5-&gt;&gt;Camera: Capture Object Position\nCamera--&gt;&gt;UR5: Return Object Coordinates\nUR5-&gt;&gt;TurtleBot: Send Object Coordinates\nTurtleBot-&gt;&gt;SLAM: Update Map and Plan Path\nTurtleBot-&gt;&gt;Navigation: Move Towards Object Location\nNavigation--&gt;&gt;TurtleBot: Arrived at Target\nTurtleBot-&gt;&gt;UR5: Object Located Successfully</code></pre>"},{"location":"second-page/","title":"Second Page","text":"<p>Things to discuss</p>"},{"location":"component-selection-example/","title":"Component Selection Example","text":""},{"location":"component-selection-example/#examples","title":"Examples","text":""},{"location":"component-selection-example/#style-1","title":"Style 1","text":"<p>This is the example found in the assignment, uses more html</p> <p>Table 1: Example component selection</p> <p>External Clock Module</p> Solution Pros Cons Option 1. XC1259TR-ND surface mount crystal$1/eachlink to product * Inexpensive[^1]* Compatible with PSoC* Meets surface mount constraint of project * Requires external components and support circuitry for interface* Needs special PCB layout. * Option 2. * CTX936TR-ND surface mount oscillator * $1/each * Link to product * Outputs a square wave * Stable over operating temperature  * Direct interface with PSoC (no external circuitry required) range * More expensive * Slow shipping speed <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"component-selection-example/#style-2","title":"Style 2","text":"<p>Also acceptable, more markdown friendly</p> <p>External Clock Module</p> <ol> <li> <p>XC1259TR-ND surface mount crystal</p> <p></p> <ul> <li>$1/each</li> <li>link to product</li> </ul> Pros Cons Inexpensive Requires external components and support circuitry for interface Compatible with PSoC Needs special PCB layout. Meets surface mount constraint of project </li> <li> <p>CTX936TR-ND surface mount oscillator</p> <p></p> <ul> <li>$1/each</li> <li>Link to product</li> </ul> Pros Cons Outputs a square wave More expensive Stable over operating temperature Slow shipping speed Direct interface with PSoC (no external circuitry required) range </li> </ol> <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\n  loader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\n  const svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\n  console.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"subfolder/","title":"This is the index to a subfolder","text":"<p>Things to discuss</p>"},{"location":"subfolder/about/","title":"About Team05","text":"<p>Welcome to the official website of Team05! We are a group of passionate students dedicated to advancing robotics technology through experimentation and deployment.</p>","tags":["team","mission","project"]},{"location":"subfolder/about/#our-mission","title":"Our Mission","text":"<p>Our mission is to innovate and push the boundaries of robotics by developing cutting-edge systems and deploying them in real-world scenarios. We aim to create solutions that are both practical and impactful.</p>","tags":["team","mission","project"]},{"location":"subfolder/about/#meet-the-team","title":"Meet the Team","text":"","tags":["team","mission","project"]},{"location":"subfolder/about/#john-doe","title":"John Doe","text":"<ul> <li>Role: Team Lead</li> <li>Expertise: Robotics, Embedded Systems</li> <li>Bio: John is a senior student with a passion for robotics and automation. He oversees the team's projects and ensures everything runs smoothly.</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#jane-smith","title":"Jane Smith","text":"<ul> <li>Role: Software Developer</li> <li>Expertise: Machine Learning, AI</li> <li>Bio: Jane specializes in developing intelligent algorithms for robotics systems. She is responsible for the software architecture of our projects.</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#alex-johnson","title":"Alex Johnson","text":"<ul> <li>Role: Hardware Engineer</li> <li>Expertise: Circuit Design, Sensors</li> <li>Bio: Alex focuses on designing and building the hardware components of our robotics systems. He ensures that our designs are both efficient and reliable.</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#our-projects","title":"Our Projects","text":"","tags":["team","mission","project"]},{"location":"subfolder/about/#project-1-autonomous-navigation","title":"Project 1: Autonomous Navigation","text":"<ul> <li>Description: Developing an autonomous robot capable of navigating complex environments.</li> <li>Technologies: ROS, SLAM, Computer Vision</li> <li>Status: In Progress</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#project-2-robotic-arm","title":"Project 2: Robotic Arm","text":"<ul> <li>Description: Building a robotic arm for industrial automation.</li> <li>Technologies: Arduino, Servo Motors, Kinematics</li> <li>Status: Completed</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#our-achievements","title":"Our Achievements","text":"<ul> <li>2023 Robotics Competition: 1st Place</li> <li>2022 Innovation Award: Best Robotics Project</li> <li>2021 Hackathon: Top 5 Finalists</li> </ul>","tags":["team","mission","project"]},{"location":"subfolder/about/#contact-us","title":"Contact Us","text":"<p>If you have any questions or would like to collaborate with us, feel free to reach out:</p> <ul> <li>Email: team05@example.com</li> <li>GitHub: Team05 GitHub</li> <li>Website: Team05 Website</li> </ul> <p>\"Innovation is the key to the future.\" \u2013 Team05</p>","tags":["team","mission","project"]},{"location":"subfolder/another-subfile/","title":"This is a secondary sub page","text":"<p>Things to discuss</p>"}]}